{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/taobrienlbl/advanced_earth_science_data_analysis/blob/spring_2023_iub/lessons/07_machine_learning_intro/07_workalong01_machine_learning_basics.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "# Machine Learning Basics\n",
    "\n",
    "This workalong walks through some of the basic elements of modern machine learning, including *rectified linear units (reLU)*, *loss functions*, *backpropagation*, and the use of `pytorch`.\n",
    "\n",
    "## Rectified linear units (reLU)\n",
    "\n",
    "In the cell below, implement two functions:\n",
    " 1. `linear(x,w,b)`\n",
    " 2. `relu(x)` \n",
    "\n",
    "as indicated in the functions' docstrings.  (Note, remove the lines with `pass` - this is just a line of code that says to Python \"do nothing\"...it's necessary because function's can't be empty in Python.)\n",
    "\n",
    "Test that it works as expected, and keep your tests in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Define a linear and relu function. \"\"\"\n",
    "import numpy as np\n",
    "\n",
    "def linear(x : np.ndarray, w : float = 1, b : float = 0) -> np.ndarray:\n",
    "    \"\"\" returns w*x + b \"\"\"\n",
    "    pass\n",
    "\n",
    "def relu(x : np.ndarray) -> np.ndarray:\n",
    "    \"\"\" returns x if x is positive, 0 otherwise\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Test the relu function. \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a function with reLUs\n",
    "\n",
    "1. Run the cell below to create a graph of fake, noisy data (variable `noisy_data`).\n",
    "1. In the cell below that, create a function `relu_sum` that is the sum of two reLUs with a bias (a scalar) added at the end\n",
    "1. Create a new plot that includes the function output and the noisy data\n",
    "1. Adjust the `w` and `b` parameters until the function approximates the fake data.\n",
    "1. Comment in a markdown cell on your observations, what you learned, etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create a fake, noisy dataset. \"\"\"\n",
    "\n",
    "# define the x values\n",
    "x = np.linspace(-np.pi, np.pi, 1000)\n",
    "\n",
    "# define the y values of the true function\n",
    "y_true = -np.cos(x)\n",
    "\n",
    "# add some noise to the true function\n",
    "np.random.seed(5984)\n",
    "noisy_data = y_true + np.random.normal(0, 0.2, len(x))\n",
    "\n",
    "# plot the noisy data\n",
    "fig, ax = plt.subplots(figsize = (5,5))\n",
    "ax.plot(x, noisy_data, '.', label=\"data\")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"Noisy Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Define a function that is the sum of two relus. \"\"\"\n",
    "\n",
    "def relu_sum(x : np.ndarray, w : np.ndarray, b : np.ndarray, bias : float = 0) -> np.ndarray:\n",
    "    \"\"\" returns relu(w[0]*x + b[0]) + relu(w[1]*x + b[1]) + bias \"\"\"\n",
    "    pass\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Plot the sum of two relus, with w, b such that the function fits the data. \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "\n",
    "Now define a *loss* function $\\mathcal{L}$: in this case *mean-squared error (MSE)* for the fit $\\hat{\\mathbf{z}}$ relative to the data $\\mathbf{z}$\n",
    "\n",
    "$$ \\mathcal{L} = \\frac{1}{N} \\sum\\limits_{i=0}^{N-1} (z_i - \\hat{z}_i)^2$$\n",
    "\n",
    "Calculate the loss for your fit to the data above: we'll compare in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Define a loss function. \"\"\"\n",
    "\n",
    "def loss(y_true : np.ndarray, y_pred : np.ndarray) -> float:\n",
    "    \"\"\" returns the mean squared error between y_true and y_pred \"\"\"\n",
    "    pass\n",
    "\n",
    "# calculate the loss for the fit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the loss to the title of the plot, and vary the parameters further to see if you can minimize the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Vary parameters, minimizing the loss (include loss in title). \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLUs in `pytorch`\n",
    "\n",
    "Now we'll take advantage of `pytorch` for automating the process of fitting a NN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Define a neural network with a single hidden layer. \"\"\"\n",
    "# import pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# set the random seed for reproducibility\n",
    "torch.manual_seed(5984)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Show the initial model prediction/fit (it's bad)\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model - the simple, but not-so-great way\n",
    "\n",
    "We'll use a simple for loop to train the model on all the data.  This is bad because we don't split the dataset into train, test, and validation sets; this can make our efforts prone to overfitting.\n",
    "\n",
    "But for simplicity, we'll forge forward with this (inadvisable) approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Train the model the bad way. \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Plot the trained model. \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model - the better, but more complicated way\n",
    "\n",
    "We'll follow standard practice here to split the data into test, train and validation sets.  We'll also use pytorch's data loader to do training in batches, and we'll put our training loop into a function.  This will allow us to train in *epochs* (passes through the entire dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Train the model the better, but more complicated way. \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Plot the prediction \"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "easg690",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
